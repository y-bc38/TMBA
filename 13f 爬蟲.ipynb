{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc95f20e",
   "metadata": {},
   "source": [
    "# 13F爬蟲\n",
    "\n",
    "1. 挑出想要看的機構與其CIK     \n",
    "範例 : https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0001350694&owner=exclude&count=40\n",
    "    \n",
    "2. 輸入CIK後找出每季13f資料，並抓出其xml(通常是第4個, index = 3)  \n",
    "    範例 : https://www.sec.gov/Archives/edgar/data/1350694/000117266122001788/infotable.xml\n",
    "    \n",
    "3. 整理xml資料，輸進excel\n",
    "\n",
    "ref : https://github.com/CodeWritingCow/sec-web-scraper-13f/blob/master/scraper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "5423a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1a7d17",
   "metadata": {},
   "source": [
    "# 前置作業"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "749274ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# warnings.warn('DelftStack')\n",
    "# warnings.warn('Do not show this message')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d5c87758",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_url = 'https://www.sec.gov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "cdf18cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'HOST': 'www.sec.gov',\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c83c13",
   "metadata": {},
   "source": [
    "# 公司"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614fc122",
   "metadata": {},
   "source": [
    "## 每個公司的各自季度報告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "e16d7496",
   "metadata": {},
   "outputs": [],
   "source": [
    "#更換cik代號可抓取不同公司\n",
    "#請參考cik.csv\n",
    "\n",
    "url = \"https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000921669&type=13F-HR&dateb=&owner=exclude&count=40&search_text=\"\n",
    "r = requests.get(url, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "7ac80e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "#繳交日期\n",
    "date = []\n",
    "\n",
    "for i in range(len(soup.text.split(\"\\n\"))):\n",
    "    if len(soup.text.split(\"\\n\")[i]) == 10: \n",
    "        if(soup.text.split(\"\\n\")[i][0] == \"2\") & (soup.text.split(\"\\n\")[i][4] == \"-\"):\n",
    "            date.append(soup.text.split(\"\\n\")[i])\n",
    "\n",
    "#tags為所有報告之相對路徑\n",
    "tags = soup.findAll('a', id=\"documentsbutton\")\n",
    "\n",
    "report = []\n",
    "\n",
    "for i in range(len(tags)):\n",
    "    report.append(sec_url + tags[i]['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6ca9ca",
   "metadata": {},
   "source": [
    "## 各季度報告跟抓每季的第三個xml檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6ddaf1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-15 success\n",
      "2022-05-13 success\n",
      "2022-02-14 success\n",
      "2021-11-15 success\n",
      "2021-08-13 success\n",
      "2021-05-14 success\n",
      "2021-02-12 success\n",
      "2020-11-13 success\n",
      "2020-08-14 success\n",
      "2020-05-15 success\n",
      "2020-02-14 success\n",
      "2019-11-14 success\n",
      "2019-08-14 success\n",
      "2019-05-15 success\n",
      "2019-02-14 success\n",
      "2018-11-14 success\n",
      "2018-08-14 success\n",
      "2018-05-15 success\n",
      "2018-02-14 success\n",
      "2017-11-14 success\n",
      "2017-08-14 success\n",
      "2017-05-15 success\n",
      "2017-02-14 success\n",
      "2016-11-14 success\n",
      "2016-08-15 success\n",
      "2016-05-16 success\n",
      "2016-02-16 success\n",
      "2015-11-16 success\n",
      "2015-08-14 success\n",
      "2015-05-15 success\n",
      "2015-02-17 success\n",
      "2014-11-14 success\n",
      "2014-08-14 success\n",
      "2014-05-15 success\n",
      "2014-02-14 success\n",
      "2013-11-14 success\n",
      "2013-08-14 success\n",
      "2013-05-15 no data\n",
      "2013-02-14 no data\n",
      "2012-11-15 no data\n"
     ]
    }
   ],
   "source": [
    "xmls = []\n",
    "\n",
    "for i in range(len(report)):\n",
    "    r2 = requests.get(report[i], headers = headers)\n",
    "    s2 = BeautifulSoup(r2.text, \"html.parser\")\n",
    "    t2 = s2.findAll('a', attrs={'href': re.compile('xml')})\n",
    "    \n",
    "    try :\n",
    "        xml_url = sec_url + t2[3].get('href')\n",
    "        xmls.append(xml_url)\n",
    "            \n",
    "        print(str(date[i]) + \" success\")\n",
    "    \n",
    "    except :\n",
    "        print(str(date[i]) + \" no data\")\n",
    "    \n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "34a287af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2 = BeautifulSoup(r2.text, \"html.parser\")\n",
    "# t2 = s2.findAll('a', attrs={'href': re.compile('xml')})\n",
    "# xml_url = t2[3].get('href')\n",
    "# xml_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6cf9b0",
   "metadata": {},
   "source": [
    "## 資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a9ab234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-15 seasonal filing\n",
      "2022-05-13 seasonal filing\n",
      "2022-02-14 seasonal filing\n",
      "2021-11-15 seasonal filing\n",
      "2021-08-13 seasonal filing\n",
      "2021-05-14 seasonal filing\n",
      "2021-02-12 seasonal filing\n",
      "2020-11-13 seasonal filing\n",
      "2020-08-14 seasonal filing\n",
      "2020-05-15 seasonal filing\n",
      "2020-02-14 seasonal filing\n",
      "2019-11-14 seasonal filing\n",
      "2019-08-14 seasonal filing\n",
      "2019-05-15 seasonal filing\n",
      "2019-02-14 seasonal filing\n",
      "2018-11-14 seasonal filing\n",
      "2018-08-14 seasonal filing\n",
      "2018-05-15 seasonal filing\n",
      "2018-02-14 seasonal filing\n",
      "2017-11-14 seasonal filing\n",
      "2017-08-14 seasonal filing\n",
      "2017-05-15 seasonal filing\n",
      "2017-02-14 seasonal filing\n",
      "2016-11-14 seasonal filing\n",
      "2016-08-15 seasonal filing\n",
      "2016-05-16 seasonal filing\n",
      "2016-02-16 seasonal filing\n",
      "2015-11-16 seasonal filing\n",
      "2015-08-14 seasonal filing\n",
      "2015-05-15 seasonal filing\n",
      "2015-02-17 seasonal filing\n",
      "2014-11-14 seasonal filing\n",
      "2014-08-14 seasonal filing\n",
      "2014-05-15 seasonal filing\n",
      "2014-02-14 seasonal filing\n",
      "2013-11-14 seasonal filing\n",
      "2013-08-14 seasonal filing\n"
     ]
    }
   ],
   "source": [
    "a = pd.DataFrame(columns = ['date', 'Name of Issuer', \"Value (x$1000)\"])\n",
    "\n",
    "for i in range(len(xmls)):\n",
    "    \n",
    "#     #臨時更換投資組合需跳過\n",
    "#     if (i == 4) | (i == 30) :\n",
    "        \n",
    "#         print(str(date[i]) + \" is a provisional declaration\")\n",
    "        \n",
    "#         continue\n",
    "    \n",
    "    #每筆資料\n",
    "    r3 = requests.get(xmls[i], headers = headers)\n",
    "    soup_xml = BeautifulSoup(r3.content, \"lxml\")\n",
    "\n",
    "    issuers = soup_xml.body.findAll(re.compile('nameofissuer'))\n",
    "    values = soup_xml.body.findAll(re.compile('value'))\n",
    "\n",
    "    data = pd.DataFrame(columns = ['Name of Issuer', \"Value (x$1000)\"])\n",
    "\n",
    "    for issuer, value in zip(issuers, values):\n",
    "\n",
    "        row = {\n",
    "\n",
    "            \"Name of Issuer\": issuer.text,\n",
    "            \"Value (x$1000)\": float(value.text),\n",
    "\n",
    "                }\n",
    "\n",
    "        data = data.append(row, ignore_index = True)\n",
    "\n",
    "    data = data.groupby(\"Name of Issuer\").sum().reset_index()\n",
    "    data['porpotion'] = np.round(data['Value (x$1000)'] / data['Value (x$1000)'].sum(), 4) *100\n",
    "    data = data.sort_values('porpotion', ascending = False).head(20)\n",
    "    data['date'] = date[i]\n",
    "\n",
    "    \n",
    "    a = pd.concat([a, data], axis = 0)\n",
    "    print(str(date[i]) + \" seasonal filing\")\n",
    "\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "6baef6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv(\"C:\\\\Users\\\\internuser1\\\\Desktop\\\\13f 爬蟲\\\\Icahn Carl C.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71992a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8437a88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6e35767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.groupby(\"Name of Issuer\").sum().reset_index()\n",
    "# data['porpotion'] = np.round(data['Value (x$1000)'] / data['Value (x$1000)'].sum(), 4) *100\n",
    "# data = data.sort_values('porpotion', ascending = False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "95fc99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r3 = requests.get(a, headers = headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8356ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup_xml = BeautifulSoup(r3.content, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8e746aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# issuers = soup_xml.body.findAll(re.compile('nameofissuer'))\n",
    "# cusips = soup_xml.body.findAll(re.compile('cusip'))\n",
    "# values = soup_xml.body.findAll(re.compile('value'))\n",
    "# sshprnamts = soup_xml.body.findAll('sshprnamt')\n",
    "# sshprnamttypes = soup_xml.body.findAll(re.compile('sshprnamttype'))\n",
    "# investmentdiscretions = soup_xml.body.findAll(re.compile('investmentdiscretion'))\n",
    "# soles = soup_xml.body.findAll(re.compile('sole'))\n",
    "# shareds = soup_xml.body.findAll(re.compile('shared'))\n",
    "# nones = soup_xml.body.findAll(re.compile('none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "422aba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# berksha = pd.DataFrame(columns = ['Name of Issuer', \"Value (x$1000)\"])\n",
    "\n",
    "# for issuer, value in zip(issuers, values):\n",
    "        \n",
    "#     row = { \n",
    "#             \"Name of Issuer\": issuer.text,\n",
    "#             \"Value (x$1000)\": float(value.text),\n",
    "            \n",
    "#             }\n",
    "    \n",
    "#     berksha = berksha.append(row, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "051102f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# berksha = berksha.groupby(\"Name of Issuer\").sum().reset_index()\n",
    "# berksha['porpotion'] = np.round(berksha['Value (x$1000)'] / berksha['Value (x$1000)'].sum(), 4) *100\n",
    "# berksha = berksha.sort_values('porpotion', ascending = False).head(20)\n",
    "# berksha['date'] = \"2022-08-14\"\n",
    "# berksha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
